{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# Exercise\n","\n","Aim is to creating your own FlowersDataset.\n","First download the dataset you will work on using the code:\n","\n","``` %%bash\n","mkdir ./flowers102/\n","wget https://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat -P ./flowers102/\n","wget https://www.robots.ox.ac.uk/~vgg/data/flowers/102/setid.mat -P ./flowers102/\n","wget https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz -P ./flowers102/\n","tar zxvf ./flowers102/102flowers.tgz -C ./flowers102/\n","rm ./flowers102/102flowers.tgz\n","```\n","\n","You need to use the Dataloader to iterate through the samples of train and test sets and visualize them.\n","(In this case we omit the validation set for simplicity.)\n","\n","Remember the ids are in `flowers102/setid.mat`.\n","\n","You need to apply following:\n","\n","| **Operation** | **Train** | **Test** |\n","| --- | --- | --- |\n","| Normalize and Resize | ✅ | ✅ |\n","| Data Augmenation | ✅ | ⛔ |\n","| Shuffle Dataloader | ✅ | ⛔ |\n","\n","\n","Useful\n","torchvision transformations can e found in: https://pytorch.org/vision/stable/transforms.html\n","\n","HINTS:\n","\n","a) You need to create the dataset class that can handle training and test.\n","\n","b)\tYou need to split the data according to the partition (use ```setid.mat```)\n","\n","c)\t```io.loadmat``` is to load all the ids. Recall that the ids in ```setid.mat``` of FlowerDataset 1-indexed.\n","To train a model you need to subtract “1” from the indexes supplied.\n","\n","d)\tFor training apply any augmentation you want (for example: ```RandomHorizontalFlip```, ```RandomInvert```)\n","\n","e)\tUse batch size 8 to use DataLoader\n","\n","f)\tNotice that you need to separate train and test DataLoaders\n"],"metadata":{"id":"NVBwbNBmkGvX"}},{"cell_type":"markdown","source":["## SOLUTION"],"metadata":{"id":"xJGL05Qd-CFW"}},{"cell_type":"code","source":["# download the dataset we will work with\n","%%bash\n","mkdir ./flowers102/\n","wget https://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat -P ./flowers102/\n","wget https://www.robots.ox.ac.uk/~vgg/data/flowers/102/setid.mat -P ./flowers102/\n","wget https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz -P ./flowers102/\n","tar zxvf ./flowers102/102flowers.tgz -C ./flowers102/\n","rm ./flowers102/102flowers.tgz"],"metadata":{"id":"I7cxi7OvCk9M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torchvision.transforms as transforms\n","import torchvision\n","import torch\n","import PIL.Image as Image\n","import matplotlib.pyplot as plt\n","import os\n","import scipy.io as io"],"metadata":{"id":"EAlAjG4NwWpx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Create the dataset calss, which can handle training and test apporpriately.\n","\n","class MyDataset(torch.utils.data.Dataset):\n","\n","  def __init__(self, root: str, img_size: int, partition: str = \"train\"):\n","\n","    assert partition in [\"train\", \"test\"], print(\"Partition should be train or test\")\n","    self.root = root\n","\n","    # === Indices (samples)\n","    # Split data according to the training and test\n","\n","    ids = io.loadmat(os.path.join(root, \"setid.mat\")) # Load all the ids\n","    if partition == \"train\":\n","      self.ids = ids[\"trnid\"][0]\n","    else:\n","      self.ids = ids[\"tstid\"][0]\n","\n","    # === Labels\n","    # Load the labels corresponding to the images\n","    all_labels = io.loadmat(os.path.join(root, \"imagelabels.mat\"))[\"labels\"][0] # Load all the labels\n","    self.labels = all_labels[self.ids - 1] # Keep only the labels associated with the current IDs.\n","    # setid.mat is based on index 1. Subtract one to match it to index 0.\n","    # Keep only the labels associated to current ids. setid.mat is 1 indexed we subtract one to make it 0 index.\n","\n","    # === Transformations\n","    if partition == \"train\":\n","      # Only for training\n","      self.T = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.RandomCrop(size=(img_size, img_size)),\n","            transforms.Resize(size=(img_size, img_size)),\n","            transforms.Normalize(mean=(0.0, 0.0, 0.0), std=(1.0, 1.0, 1.0)),  # normalize [0, 1]\n","            transforms.RandomHorizontalFlip(p=0.5),\n","            transforms.RandomGrayscale(p=0.2),\n","            transforms.RandomInvert(p=0.2),\n","\n","      ])\n","    elif partition == \"test\":\n","      # Only for testing\n","      self.T = transforms.Compose([\n","                transforms.ToTensor(),\n","                transforms.Resize(size=(img_size, img_size)),\n","                transforms.Normalize(mean=(0.0, 0.0, 0.0), std=(1.0, 1.0, 1.0)),  # normalize [0, 1]\n","            ])\n","\n","\n","  def __len__(self):\n","    return self.ids.shape[0]\n","\n","  def __getitem__(self, idx):\n","\n","    img_name = f\"image_{str(self.ids[idx]).zfill(5)}.jpg\"\n","    img_path = os.path.join(self.root, \"jpg\", img_name)\n","    img = Image.open(img_path).convert(\"RGB\")\n","    img = self.T(img)\n","\n","    labels = self.labels[idx]\n","    return img, labels"],"metadata":{"id":"KlFgF8vb8wVp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_root = \"flowers102/\"\n","batch_size = 8\n","image_size = 256\n","\n","\n","# Dataset and DataLoader for training\n","train_dataset = MyDataset(root=data_root,\n","                          img_size=image_size,\n","                          partition=\"train\")\n","\n","train_dataloader = torch.utils.data.DataLoader(train_dataset,\n","                                        batch_size=batch_size,\n","                                        shuffle=True)"],"metadata":{"id":"ZCtgmWFd_tH6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Dataset and DataLoader for testing\n","test_dataset = MyDataset(root=data_root,\n","                         img_size=256,\n","                         partition=\"test\")\n","test_dataloader = torch.utils.data.DataLoader(test_dataset,\n","                                        batch_size=1,\n","                                        shuffle=False)\n"],"metadata":{"id":"N6dPDKtJ5BFY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualize training samples\n","print(f\"Training dataset contains: {len(train_dataset)} samples\")\n","train_batch, _ = next(iter(train_dataloader))\n","train_img = torchvision.utils.make_grid(train_batch)\n","\n","f = plt.figure(figsize=(30, 5))\n","plt.imshow(train_img.permute(1,2,0))\n","plt.axis(\"off\")\n","plt.title(\"Training Images\")\n","plt.show()"],"metadata":{"id":"H7i6bHdIASXx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualize testing samples\n","print(f\"Test dataset contains: {len(test_dataset)} samples\")\n","test_batch, _ = next(iter(test_dataloader))\n","test_img = torchvision.utils.make_grid(test_batch)\n","\n","f = plt.figure(figsize=(30, 5))\n","plt.imshow(test_img.permute(1,2,0))\n","plt.axis(\"off\")\n","plt.title(\"Test Images\")\n","plt.show()"],"metadata":{"id":"0wtGWz9KBslp"},"execution_count":null,"outputs":[]}]}