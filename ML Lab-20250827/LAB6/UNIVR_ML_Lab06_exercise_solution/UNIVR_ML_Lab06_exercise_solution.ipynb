{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# Exercise: Principal Component Analysis (PCA) and Classification Performance\n","\n","In this exercise, you will apply PCA to reduce the dimensionality of the Olivetti Faces Dataset.\n","\n","You will then train and test various classifiers on the PCA-reduced data.\n","\n","The goal is to explore how different classifiers perform with different amounts of explained variance in PCA.\n","\n","1) Load the Olivetti Faces Dataset.\n","2) Apply PCA with varying numbers of components to capture different levels of explained variance (e.g., 80%, 90%, and 95%).\n","3) For each level of explained variance, reduce the dataset's dimensionality using PCA and train the following classifiers on the transformed data:\n","- k-Nearest Neighbors (k-NN)\n","- Parzen Window Classifier (use Gaussian kernel density estimate)\n","- Logistic Regression\n","\n","4) Compare the classification accuracy of each classifier across the different levels of explained variance.\n","\n","5) Analyze and discuss:\n","5.1) Which classifier performs best with fewer PCA components and why?\n","5.2) How does the number of components (variance retained) affect each classifierâ€™s performance?"],"metadata":{"id":"ukeNtuBnPkFn"}},{"cell_type":"code","source":["from sklearn.datasets import fetch_olivetti_faces\n","from sklearn.model_selection import train_test_split\n","from sklearn.decomposition import PCA\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KernelDensity\n","from sklearn.metrics import accuracy_score\n","import numpy as np\n","import pandas as pd\n","\n","# Load dataset\n","faces = fetch_olivetti_faces()\n","X, y = faces.data, faces.target\n","\n","# Split into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Define variance levels for PCA and create dictionaries to store results\n","variance_levels = [0.50, 0.70, 0.90, 0.95]\n","results = {var: {} for var in variance_levels}  # Initialize results dictionary\n","\n","# Apply PCA and perform classification at each variance level\n","for var in variance_levels:\n","    # Perform PCA with the specified variance level\n","    pca = PCA(var)\n","    X_train_pca = pca.fit_transform(X_train)\n","    X_test_pca = pca.transform(X_test)\n","\n","    # k-Nearest Neighbors (k=5)\n","    knn = KNeighborsClassifier(n_neighbors=5)\n","    knn.fit(X_train_pca, y_train)\n","    y_pred_knn = knn.predict(X_test_pca)\n","    results[var]['k-NN'] = accuracy_score(y_test, y_pred_knn)\n","\n","    # Parzen Window Classifier with Kernel Density Estimation\n","    bandwidth = 1.0  # You can experiment with different bandwidths\n","    classes = np.unique(y_train)\n","    log_densities = []\n","\n","    for cls in classes:\n","        # Filter training data by class: this example considers that each class has a unique distribution\n","        X_train_class = X_train_pca[y_train == cls]\n","        kde = KernelDensity(kernel='gaussian', bandwidth=bandwidth)\n","        kde.fit(X_train_class)\n","\n","        # Score samples in the test set (log-density for each class)\n","        log_density = kde.score_samples(X_test_pca)\n","        log_densities.append(log_density)\n","\n","    # Convert list to array and find class with highest log-density for each sample\n","    log_densities = np.array(log_densities).T  # Shape: [n_samples, n_classes]\n","    y_pred_parzen = classes[np.argmax(log_densities, axis=1)]\n","    results[var]['Parzen Window'] = accuracy_score(y_test, y_pred_parzen)\n","\n","    # Logistic Regression\n","    logistic = LogisticRegression(max_iter=1000)\n","    logistic.fit(X_train_pca, y_train)\n","    y_pred_logistic = logistic.predict(X_test_pca)\n","    results[var]['Logistic Regression'] = accuracy_score(y_test, y_pred_logistic)\n","\n","# Display the results in a DataFrame for easy viewing\n","results_df = pd.DataFrame(results).T\n","print(\"Classification Accuracy for Different PCA Variance Levels and Classifiers:\\n\")\n","print(results_df)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T72OCTcuRjge","executionInfo":{"status":"ok","timestamp":1730574437300,"user_tz":-60,"elapsed":2162,"user":{"displayName":"Cigdem Beyan","userId":"00242028384913531025"}},"outputId":"ef37d107-7f7c-4b4f-c015-0d9a029860f0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Classification Accuracy for Different PCA Variance Levels and Classifiers:\n","\n","          k-NN  Parzen Window  Logistic Regression\n","0.50  0.491667       0.725000             0.625000\n","0.70  0.633333       0.833333             0.841667\n","0.90  0.775000       0.866667             0.941667\n","0.95  0.783333       0.875000             0.941667\n"]}]}]}